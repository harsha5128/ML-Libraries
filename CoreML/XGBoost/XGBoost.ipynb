{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is XGBoost?\n",
    "\n",
    "XGBoost is an optimized gradient boosting algorithm designed for speed and performance. It is widely used in ML competitions (e.g., Kaggle) due to its efficiency and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Key Features of XGBoost\n",
    "\n",
    "Boosted Trees Algorithm: Uses gradient boosting to improve weak learners (decision trees).\n",
    "\n",
    "Regularization: Has L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.\n",
    "\n",
    "Parallel Processing: Supports multi-threading, making training very fast.\n",
    "\n",
    "Handling Missing Values: Automatically learns the best direction for missing values.\n",
    "\n",
    "Tree Pruning: Uses \"max_depth\" instead of pre-pruning, which makes training efficient.\n",
    "\n",
    "Supports Distributed Computing: Can run on Hadoop, Spark, and GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Concepts\n",
    "\n",
    "1. Boosting\n",
    "\n",
    "A sequential learning method where weak learners (shallow decision trees) are improved iteratively.\n",
    "\n",
    "Each new tree corrects the errors made by previous trees.\n",
    "\n",
    "2. Gradient Boosting\n",
    "\n",
    "Models the residual errors and minimizes them using gradient descent.\n",
    "\n",
    "In each step, new trees are added to predict the residuals.\n",
    "\n",
    "3. Objective Function\n",
    "\n",
    "Includes a loss function (e.g., log loss, squared error) and a regularization term.\n",
    "\n",
    "4. Regularization\n",
    "\n",
    "L1 and L2 penalties to prevent overfitting.\n",
    "\n",
    "Helps improve model generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Key Functions\n",
    "\n",
    "Here’s a breakdown of essential functions in XGBoost:\n",
    "\n",
    "1. DMatrix\n",
    "\n",
    "A specialized data structure optimized for XGBoost.\n",
    "\n",
    "Efficient for memory usage and fast computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample data\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "labels = np.array([0, 1, 0])\n",
    "\n",
    "# Convert to DMatrix\n",
    "dtrain = xgb.DMatrix(data, label=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Training an XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",  # Classification problem\n",
    "    \"eval_metric\": \"logloss\",  # Loss function\n",
    "    \"eta\": 0.1,  # Learning rate\n",
    "    \"max_depth\": 3,  # Tree depth\n",
    "}\n",
    "\n",
    "# Train model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33924362 0.33924362 0.33924362]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "preds = bst.predict(dtrain)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Feature Importance\n",
    "\n",
    "XGBoost allows feature importance visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Sample dataset\n",
    "X, y = np.random.rand(100, 5), np.random.randint(0, 2, 100)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBClassifier\n",
    "model = XGBClassifier( eval_metric=\"logloss\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract booster\n",
    "booster = model.get_booster()\n",
    "\n",
    "# Plot feature importance\n",
    "xgb.plot_importance(booster)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Cross-Validation\n",
    "\n",
    "Use XGBoost’s built-in cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:04:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:52: Empty dataset at worker: 0\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.557281</td>\n",
       "      <td>0.216658</td>\n",
       "      <td>0.702644</td>\n",
       "      <td>0.776683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-logloss-mean  train-logloss-std  test-logloss-mean  test-logloss-std\n",
       "0            0.557281           0.216658           0.702644          0.776683\n",
       "1            0.557281           0.216658           0.702644          0.776683\n",
       "2            0.557281           0.216658           0.702644          0.776683\n",
       "3            0.557281           0.216658           0.702644          0.776683\n",
       "4            0.557281           0.216658           0.702644          0.776683\n",
       "5            0.557281           0.216658           0.702644          0.776683\n",
       "6            0.557281           0.216658           0.702644          0.776683\n",
       "7            0.557281           0.216658           0.702644          0.776683\n",
       "8            0.557281           0.216658           0.702644          0.776683\n",
       "9            0.557281           0.216658           0.702644          0.776683"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.cv(params, dtrain, num_boost_round=10, nfold=5, metrics=\"logloss\", as_pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Sklearn API (Simpler Interface)\n",
    "\n",
    "XGBoost has a Scikit-learn compatible API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create sample dataset\n",
    "X, y = np.random.rand(100, 5), np.random.randint(0, 2, 100)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = XGBClassifier(eval_metric=\"logloss\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning in XGBoost\n",
    "\n",
    "Key Parameters:\n",
    "\n",
    "n_estimators: Number of boosting rounds.\n",
    "\n",
    "max_depth: Depth of each tree.\n",
    "\n",
    "learning_rate (eta): Controls step size in optimization.\n",
    "\n",
    "subsample: Fraction of data used per boosting round.\n",
    "\n",
    "colsample_bytree: Fraction of features used per tree.\n",
    "\n",
    "gamma: Minimum loss reduction required to make a split.\n",
    "\n",
    "lambda & alpha: L2 and L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning Example (Using GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 200}\n",
      "Best Score: 0.525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(XGBClassifier(), param_grid, scoring='accuracy', cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to Use XGBoost?\n",
    "\n",
    "✅ Large datasets with tabular data\n",
    "✅ Structured data problems\n",
    "✅ Binary & multi-class classification, regression\n",
    "✅ Winning Kaggle competitions\n",
    "✅ When overfitting is an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When NOT to Use XGBoost?\n",
    "\n",
    "❌ When data is small (can overcomplicate things)\n",
    "❌ For image processing (use CNNs instead)\n",
    "❌ For NLP tasks (use Transformers/BERT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison: XGBoost vs Random Forest\n",
    "\n",
    "Feature\t            XGBoost\t                                    Random Forest\n",
    "Training            Speed\tFaster (Parallelized, Pruning)\t       Slower\n",
    "Overfitting\t        Less prone due to regularization\t           Can overfit with deep trees\n",
    "Interpretability\tModerate\t                                   Easier to interpret\n",
    "Performance\t        Often better on structured data\t               Good, but may not beat XGBoost\n",
    "Memory Usage\t    Higher due to boosting\t                       Lower\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "XGBoost is a powerful gradient boosting library optimized for efficiency.\n",
    "\n",
    "It excels in structured data and large datasets.\n",
    "\n",
    "Has many tuning parameters to optimize model performance.\n",
    "\n",
    "Integrated with Scikit-learn and supports GPU acceleration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
